{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 300\n",
    "\n",
    "from grid import GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid-World RL\n",
    "\n",
    "We're first going to get acquainted with the two flavors of RL we learned in class (Q-learning and Policy Gradient). To keep things simple, we'll use a Grid World environment; the same one you saw in the lectures.\n",
    "\n",
    "First, let's create a gridworld environment object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this environment looks like beyond the code. We can use the `render` method to see it.\n",
    "\n",
    "Our grid world has 4 columns (x-axis) and 3 rows (y-axis). The blue circle is the agent and it starts at $(x, y) = (0, 1)$. The \"objective\" state is the one with the coin, $(x, y) = (3, 0)$ and the death state is at $(x, y) = (3, 1)$. The reward of going to any state is $-0.05$, except the objective ($r=1$) and death states ($r=-1$).\n",
    "\n",
    "Also, there is a wall (or filled space) at $(x, y) = (1, 1)$. So you can not move to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to map our actions to numbers to work with them. There are 4 actions in this environment: \n",
    "* Right ($a=0$)\n",
    "* Down ($a=1$)\n",
    "* Left ($a=2$)\n",
    "* Up ($a=3$)\n",
    "\n",
    "If we go in a direction that is blocked (hitting a wall), we don't move.\n",
    "\n",
    "Try running the script below, which goes through a movement sequence and visulizes it. We can give actions to the environment through the `step` method.\n",
    "\n",
    "Everytime you want to start from the begining, you have to use the `reset` method to start over.\n",
    "\n",
    "Note: The methods that we are using here conform to the standard Atari Gym API (from the DQN paper). This API has become best practice for working with RL environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reset environment, get the initial state\n",
    "# The state in grid world is two dimenional: [x, y], where x shows the column and y the row, counting from the top left\n",
    "state = env.reset()\n",
    "\n",
    "# Map action indices to text\n",
    "direction = {0: 'right', 1: 'down', 2: 'left', 3: 'up'}\n",
    "plt.figure()\n",
    "\n",
    "# Go through a sequence of actions\n",
    "for t, action in enumerate([3, 0, 0, 1, 1, 2, 2, 0, 0, 0, 3]):\n",
    "    \n",
    "    # Take a step by choosing an action. You get 4 outputs from this:\n",
    "    #      next_state: The next state that taking the action leads to.\n",
    "    #      rew: The reward we got from taking the action\n",
    "    #      done: Is the episode over? In this environment an episode is over when we reach the objective or death states\n",
    "    #      info: Other info of interest\n",
    "    next_state, rew, done, info = env.step(action)\n",
    "    \n",
    "    # Render the environment\n",
    "    env.render(plt.gca())\n",
    "    plt.xlabel(f'Step {t}: went {direction[action]}, is in state (x, y) = ({next_state[0]}, {next_state[1]}), got a reward {rew}')\n",
    "    \n",
    "    # Some Jupyter notebook nonsense to show plots in a loop\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    time.sleep(1.0)\n",
    "\n",
    "time.sleep(3)\n",
    "plt.xlabel('Done!!!')\n",
    "display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q-Learning\n",
    "\n",
    "In this first assignment, we are going to understand and implement Q-learning in our simple grid world. Remember that Q-learning works by applying a greedy update over the current Q-values:\n",
    "\n",
    "$$Q(s, a) = \\mathbb{E}_{(r, s')~\\sim~\\epsilon(\\cdot|s, a)} \\left[ r + \\gamma \\cdot \\max_{a' \\in \\mathcal{A}}  Q(s', a') \\right]$$\n",
    "\n",
    "If this at all does not seem reasonable or familiar, you are urged to go through the lecture slides and get a clear understanding of this formula. You will need this understanding as you are going to implement it below.\n",
    "\n",
    "\n",
    "The training process for Q-learning is as follows:\n",
    "1. Take an action according to an epsilon-greedy policy; meaning that with a probability $\\epsilon$, take a random policy and otherwise choose the action that has the highest Q-value in that state.\n",
    "2. Use the greedy update rule to update your Q-value table\n",
    "3. Over time, anneal $\\epsilon$. Start with fully random choices ($\\epsilon=1$) at the start of training and decay it overtime to some low value, like 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "Complete the two code snippets below and get a working Q-Learning training process. The policy (visualized by arrows) should be optimal by the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Some hyperparameters used in the training process\n",
    "# DO NOT CHANGE\n",
    "NUM_EPOCHS = 10                    #  How many iterations (or epochs) will we train\n",
    "INITIAL_RANDOM_EPOCHS = 5          #  For the eps-greedy exploration process, how many initial epochs will we be completely random.\n",
    "                                   #  People usually do this to bootstrap their training from an initial random policy\n",
    "REPORT_EPOCHS = 2                  #  Visualize the policy after this many epochs\n",
    "MIN_EPS = 0.1                      #  Anneal epsilon from epsilon greedy to this value. Basically, this means that by the end we will alway take 10% of actions randomly\n",
    "GAMMA = 0.9                        #  This is the gamma you have seen many times with rewards and returns\n",
    "\n",
    "# Initialize a Q-value table. You can try initializing it from a random starting point, and see for yourself that it works no matter what values you start with.\n",
    "qtable = np.zeros((4, 3, 4))\n",
    "# This is the schedule for epsilon. It seems very complicated but it is simply a linear function that is clipped to a range of 0.1 and 1\n",
    "eps = lambda step: np.clip(NUM_EPOCHS/(NUM_EPOCHS-INITIAL_RANDOM_EPOCHS)*(1-step/NUM_EPOCHS), a_max=1, a_min=MIN_EPS)\n",
    "# This is the learning rate we use. Remember that learning rate has some special conditions on it for Tabular Q-learning. \n",
    "# We advise you to look over the lecture slides amd figure out why this learning rate function is a good choice\n",
    "lr = lambda step: 1/(step+1)\n",
    "\n",
    "# A random number generator. We use this to make our experiments reproducible. This is very good practice in RL training.\n",
    "rng = np.random.RandomState(seed=3)\n",
    "\n",
    "env = GridWorld()\n",
    "\n",
    "for epoch in trange(NUM_EPOCHS):\n",
    "    # Each epoch is one full episode\n",
    "    # An episode starts from the initial state (hence the reset function) and ends when we reach a terminal state, like the objective or death state.\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # We continue an episode until it terminates\n",
    "    while not done:\n",
    "        \n",
    "        ########################### START OF ASSIGNMENT 1 ###################################\n",
    "        ############# Your code below #################\n",
    "        # Write the following code to choose the action. Remember that the policy is epsilon-greedy.\n",
    "        # Use rng for random numbers\n",
    "        \n",
    "        # act = ...\n",
    "        \n",
    "        ########################### END OF ASSIGNMENT 1 #####################################\n",
    "        \n",
    "        next_obs, rew, done, info = env.step(act)\n",
    "        \n",
    "        alpha = lr(epoch)\n",
    "        ########################### START OF ASSIGNMENT 1 ###################################\n",
    "        ############# Your code below #################\n",
    "        # Write the following code to apply the greedy Q-learning update\n",
    "        # Use alpha as the learning rate for this step\n",
    "        \n",
    "        # qtable = ...\n",
    "        \n",
    "        ########################### END OF ASSIGNMENT 1 #####################################\n",
    "        \n",
    "        obs = next_obs\n",
    "    \n",
    "    # Visualize the policy based on the Q-tables\n",
    "    if epoch % REPORT_EPOCHS == 0:\n",
    "        ax = env.visualize_qtable(qtable)\n",
    "        ax.set_title(f'Epoch {epoch}')\n",
    "\n",
    "# Visualize the final policy\n",
    "ax = env.visualize_qtable(qtable)\n",
    "ax.set_title(f'Final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Policy Gradient\n",
    "\n",
    "In the second assignment, we are going to use policy gradient instead. Policy gradient works by using some policy characterized by $\\theta$, to sample trajectories. The expected return of those trajectories look like this:\n",
    "\n",
    "$$J(\\theta)=\\mathbb{E}_{\\tau \\sim p(\\tau;\\theta)} \\left[ R(\\tau) \\right] = \\sum_{\\tau} p(\\tau; \\theta) R(\\tau)$$\n",
    "\n",
    "After defining this function, policy gradient improves the returns by moving the parameters $\\theta$ in the direction of $\\nabla_\\theta J(\\theta)$. This is called gradient ascent. The policy gradient is thus:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta)=\\mathbb{E}_{\\tau \\sim p(\\tau;\\theta)} \\left[ \\nabla_\\theta \\log p(\\tau; \\theta))~R(\\tau) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulating the policy\n",
    "\n",
    "Now suppose we wanted to try this. How would we define a policy? We'll create one the same way we create a Q-value table. We'll create a policy table, where each state and action have a weight. These weights are basically our parameters, $\\theta$. So in the grid world example, for each state there are 4 numbers for 4 directions. How do we define a policy using these numbers?\n",
    "\n",
    "Say for state $s$ the numbers are $v_1(s), v_2(s), v_3(s), ..., v_M(s)$, where the number of actions are $M=|\\mathcal{A}|$. We could say the possibility of action $i$ is proportional to $v_i(s)$:\n",
    "\n",
    "$$p(a=i|s)=\\pi(i|s)=\\frac{v_i(s)}{\\sum_{a=1}^M v_a(s)}$$\n",
    "\n",
    "What is the issue with this? It restricts all $v_a(s)$ to be positive and non-zero. This is not favorable. If the policy gradient ascent causes one of them to become negative, a probability becomes negative and the whole setup fails. \n",
    "\n",
    "We'll make a small change that fixes this. We'll have the possibility of action $i$ be proportional to $e^{v_i(s)}$. This avoids the issues from before and looks like this:\n",
    "\n",
    "$$p(a=i|s)=\\pi(i|s)=\\frac{e^{v_i(s)}}{\\sum_{a=1}^M e^{v_a(s)}}$$\n",
    "\n",
    "This is called a Softmax function and is very popular in Machine Learning. The best point about all of this is that the gradient is very simple. Basically, the partial derivative of $p(a=i|s)$ with respect to any $v$ is:\n",
    "\n",
    "$$\\frac{\\partial \\pi(i|s)}{\\partial v_{\\hat{a}}~(~\\hat{s}~)} = 0,~~~~\\text{If }\\hat{s} \\neq s$$\n",
    "\n",
    "\n",
    "Since the policy in state s only depends on values that define the policy for state s. We also have:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial \\pi(i|s)}{\\partial v_{\\hat{a}}~(~\\hat{s}~)} = -\\pi(\\hat{a}|s),~~~~\\text{If }\\hat{s} = s ~ \\text{and } \\hat{a} \\neq i$$\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{\\partial \\pi(i|s)}{\\partial v_{\\hat{a}}~(~\\hat{s}~)} = 1-\\pi(i|s),~~~~\\text{If }\\hat{s} = s ~ \\text{and } \\hat{a} = i$$\n",
    "\n",
    "\n",
    "You are encouraged to derive these yourself, to gain some intuition.\n",
    "\n",
    "Note: We are omitting a few things from the standard Softmax function for simplicity. You can research the general version yourself, but this explanation should suffice for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Softmax function\n",
    "# This function takes a one-dimensional array x, and outputs the softmax probabilities.\n",
    "def softmax(x):\n",
    "    max_x = np.max(x, axis=-1, keepdims=True)\n",
    "    e_x = np.exp(x - max_x)\n",
    "    sum_x = np.sum(e_x, axis=-1, keepdims=True)\n",
    "    return e_x / sum_x\n",
    "\n",
    "# The gradient of the log of the Softmax function\n",
    "# This function takes a one-dimensional array x, and outputs a gradient vector for the parameters (x).\n",
    "def nabla_log_softmax(x, index):\n",
    "    s = -softmax(x)\n",
    "    s[index] += 1\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "Now that we have a policy, let's go over the the training process for policy gradient:\n",
    "1. Take actions according to your policy table.\n",
    "2. Once you have a full episode $\\tau$, use the policy gradient by:\n",
    "    1. Calculate the episode return $R(\\tau)$.\n",
    "    2. Calculate the gradient of the log probability $\\nabla_\\theta \\log p(\\tau; \\theta))$, for the episode. You have to loop over all actions and calculate the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "Complete the code snippet below and get a working policy-gradient training process. The policy is visualized by arrows in 4 directions, where the length of each arrow shows how likely it is.\n",
    "The policy should be somewhat optimal by the end in the top row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Some hyperparameters used in the training process\n",
    "# DO NOT CHANGE\n",
    "NUM_EPOCHS = 150                   #  How many iterations (or epochs) will we train\n",
    "REPORT_EPOCHS = 50                 #  Visualize the policy after this many epochs\n",
    "GAMMA = 0.9                        #  This is the gamma you have seen many times with rewards and returns\n",
    "\n",
    "\n",
    "# Initialize a policy table. Unlike Q-Learning, it is imperative to start with a fully zero table, as that means the starting policy is random (Why?).\n",
    "# Without this, we would not explore.\n",
    "policy_table = np.zeros((4, 3, 4))\n",
    "# This is the learning rate we use. Unlike Q-Learning, the learning rate here is fixed\n",
    "lr = 1e-1\n",
    "\n",
    "# A random number generator. We use this to make our experiments reproducible. This is very good practice in RL training.\n",
    "rng = np.random.RandomState(seed=3)\n",
    "\n",
    "env = GridWorld()\n",
    "\n",
    "for epoch in trange(NUM_EPOCHS):\n",
    "    # Each epoch is one full episode\n",
    "    # An episode starts from the initial state (hence the reset function) and ends when we reach a terminal state, like the objective or death state.\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # We have to store our interactions with the environment, so we can apply the policy gradient update afterwards. Why didn't we need this in Q-Learning?\n",
    "    list_obs = []\n",
    "    list_rew = []\n",
    "    list_act = []\n",
    "    \n",
    "    # We continue an episode until it terminates\n",
    "    while not done:\n",
    "        # Choose the next action. \"softmax(policy_table[obs[0], obs[1]])\" gives a distribution over actions.\n",
    "        # \"rng.choice\" can sample actions with that distribution\n",
    "        act = rng.choice(4, p=softmax(policy_table[obs[0], obs[1]]))\n",
    "        \n",
    "        next_obs, rew, done, info = env.step(act)\n",
    "        \n",
    "        # Store interactions\n",
    "        list_obs.append(obs)\n",
    "        list_rew.append(rew)\n",
    "        list_act.append(act)\n",
    "        \n",
    "        obs = next_obs\n",
    "    \n",
    "    ########################### START OF ASSIGNMENT 2 ###################################\n",
    "    ############# Your code below #################\n",
    "    # Write the following code to apply the policy-gradient update. We have already laid out the steps.\n",
    "    # 1. You have to compute R(tau) and save it to rew_all\n",
    "    # 2. You have to compute the gradients of log softmax policies\n",
    "    # 3. You have to apply the gradient ascent based on these two\n",
    "\n",
    "    # rew_all = ...\n",
    "\n",
    "    nabla_policy_table = np.zeros((4, 3, 4))\n",
    "\n",
    "    # for obs, act in zip(list_obs, list_act):\n",
    "    #     nabla_policy_table += ...\n",
    "\n",
    "    # policy_table += lr * ...\n",
    "\n",
    "    ########################### END OF ASSIGNMENT 2 #####################################\n",
    "    \n",
    "    # Visualize the policy\n",
    "    if epoch % REPORT_EPOCHS == 0:\n",
    "        ax = env.visualize_policy_table(softmax(policy_table))\n",
    "        ax.set_title(f'Epoch {epoch}')\n",
    "        \n",
    "\n",
    "# Visualize the final policy\n",
    "ax = env.visualize_policy_table(softmax(policy_table))\n",
    "ax.set_title(f'Final')\n",
    "\n",
    "policy_table_1 = policy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An improvement\n",
    "Remember that we could simplify the policy gradient formula by scoring an action only based on the actions that come after it. Basically, a rederivation of the of the previous policy gradient looked like this:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta)= \\mathbb{E} \\left[ \\sum_{t\\ge 0} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot (\\sum_{l\\ge0}\\gamma^l r_l) \\right]$$\n",
    "\n",
    "The rewards we assign to action $a_t$ in step $s_t$ include rewards $r_0, r_1, ... r_{t-1}$. However, $a_t$ had nothing to do with previous rewards and is only responsible for the returns after $t$. So we could make a small change in the formula:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta)= \\mathbb{E} \\left[ \\sum_{t\\ge 0} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot (\\sum_{l\\ge t}\\gamma^l r_l) \\right]$$\n",
    "\n",
    "By employing this trick, we decrease the variance of the policy gradient method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3\n",
    "Complete the code snippet below and decrease the policy gradient variance. We generally expect the policy to converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Some hyperparameters used in the training process\n",
    "# DO NOT CHANGE\n",
    "NUM_EPOCHS = 150                   #  How many iterations (or epochs) will we train\n",
    "REPORT_EPOCHS = 50                 #  Visualize the policy after this many epochs\n",
    "GAMMA = 0.9                        #  This is the gamma you have seen many times with rewards and returns\n",
    "\n",
    "\n",
    "# Initialize a policy table. Unlike Q-Learning, it is imperative to start with a fully zero table, as that means the starting policy is random (Why?).\n",
    "# Without this, we would not explore.\n",
    "policy_table = np.zeros((4, 3, 4))\n",
    "# This is the learning rate we use. Unlike Q-Learning, the learning rate here is fixed\n",
    "lr = 1e-1\n",
    "\n",
    "# A random number generator. We use this to make our experiments reproducible. This is very good practice in RL training.\n",
    "rng = np.random.RandomState(seed=3)\n",
    "\n",
    "env = GridWorld()\n",
    "\n",
    "for epoch in trange(NUM_EPOCHS):\n",
    "    # Each epoch is one full episode\n",
    "    # An episode starts from the initial state (hence the reset function) and ends when we reach a terminal state, like the objective or death state.\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # We have to store our interactions with the environment, so we can apply the policy gradient update afterwards. Why didn't we need this in Q-Learning?\n",
    "    list_obs = []\n",
    "    list_rew = []\n",
    "    list_act = []\n",
    "    \n",
    "    # We continue an episode until it terminates\n",
    "    while not done:\n",
    "        # Choose the next action. \"softmax(policy_table[obs[0], obs[1]])\" gives a distribution over actions.\n",
    "        # \"rng.choice\" can sample actions with that distribution\n",
    "        act = rng.choice(4, p=softmax(policy_table[obs[0], obs[1]]))\n",
    "        \n",
    "        next_obs, rew, done, info = env.step(act)\n",
    "        \n",
    "        # Store interactions\n",
    "        list_obs.append(obs)\n",
    "        list_rew.append(rew)\n",
    "        list_act.append(act)\n",
    "        \n",
    "        obs = next_obs\n",
    "    \n",
    "    ########################### START OF ASSIGNMENT 3 ###################################\n",
    "    ############# Your code below #################\n",
    "    # Write the following code to apply the policy-gradient update. We have already laid out the steps.\n",
    "    # 1. You have to compute the [sum of gamma^l r_l for l >= t] and save it to returns\n",
    "    # 2. You have to compute the gradients of log softmax policies times the returns from step 1 (this is different from before, why?)\n",
    "    # 3. You have to apply the gradient ascent based on step 2\n",
    "\n",
    "    traj_length = len(rew_list)\n",
    "    returns = np.zeros(traj_length)\n",
    "    nabla_policy_table = np.zeros((4, 3, 4))\n",
    "    \n",
    "    # for i in range(traj_length):\n",
    "    #     returns[i] = ...\n",
    "\n",
    "    # for obs, act in zip(list_obs, list_act):\n",
    "    #     nabla_policy_table += ...\n",
    "\n",
    "    # policy_table += lr * ...\n",
    "\n",
    "    ########################### END OF ASSIGNMENT 3 #####################################\n",
    "    \n",
    "    # Visualize the policy\n",
    "    if epoch % REPORT_EPOCHS == 0:\n",
    "        ax = env.visualize_policy_table(softmax(policy_table))\n",
    "        ax.set_title(f'Epoch {epoch}')\n",
    "        \n",
    "\n",
    "# Visualize the final policy\n",
    "ax = env.visualize_policy_table(softmax(policy_table))\n",
    "ax.set_title(f'Final')\n",
    "\n",
    "policy_table_1 = policy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the result of Assignment 2 and Assignment 3. The policy at the end of Assignment 3 should have converged faster than 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 2, figsize=(12.8, 4.8))\n",
    "env.visualize_policy_table(softmax(policy_table_1), axes[0])\n",
    "axes[0].set_title(f'Final policy in Assignment 2')\n",
    "env.visualize_policy_table(softmax(policy_table_2), axes[1])\n",
    "axes[1].set_title(f'Final policy in Assignment 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "You might have noticed, in Assignment 2 and 3, that the policies for the bottom row don't look so great. Why is this? Does it mean we are getting bad returns?\n",
    "\n",
    "\n",
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Bit Rate (ABR) and Pensieve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last part of this lab, we'll recreate [Pensieve](https://web.mit.edu/pensieve/)!!!\n",
    "\n",
    "The code snippet below runs a training process for the ABR environment (slightly simplified to converge in 750 epochs). We use Pytorch to handle neural networks and automatic gradient calculation.\n",
    "\n",
    "It is beneficial, but not obligatory, to go through this code and understand what is happening. If you are interested, you could also look at `core_pg.py` to see how the policy gradient variant in this setup works. \n",
    "\n",
    "Just briefly introducing this environment:\n",
    "\n",
    "* The point is to download video chunks in a streaming session, while optimizing for high quality video and low rebuffering.\n",
    "* The state in the ABR environment is:\n",
    "    * the current buffer occupancy\n",
    "    * previous action\n",
    "    * chunks left in the video\n",
    "    * some history of throughput and download times\n",
    "    * chunk sizes for the next video chunk\n",
    "* The actions in this environment are a set of video encodings, each different in size and quality. Naturally, a higher size means a higher quality.\n",
    "* The reward is some QoE (Quality of Experience) metric. This metric is a linear combination of quality and rebuffering penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core_pg import sample_action, train_actor_critic\n",
    "from core_log import log_a2c\n",
    "from abr_env.abr import ABRSimEnv\n",
    "from nn import PensNet\n",
    "from utils import ewma\n",
    "from buffer import TransitionBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from normer import Normer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "def train_pensieve(gamma, ent_max, ent_decay, name):\n",
    "    # Create some folders to save models and tensorboard data\n",
    "    os.makedirs(f'models/{name}/', exist_ok=True)\n",
    "    os.makedirs('tensorboard/', exist_ok=True)\n",
    "\n",
    "    # Monitor training with tensorboard\n",
    "    monitor = SummaryWriter(f'tensorboard/{name}', flush_secs=10)\n",
    "    \n",
    "    # An object to normalize the observation vectors, This usually speeds up training.\n",
    "    normer = Normer()\n",
    "\n",
    "    # A set of hyperparameters\n",
    "    # DO NOT CHANGE\n",
    "    LR = 1e-2                       #  Learning rate\n",
    "    WD = 1e-4                       #  Weight decay (or in other words, L2 regularization penalty)\n",
    "    NUM_EPOCHS = 750                #  How many epochs to train\n",
    "    EPOCH_SAVE = 100                #  How many epochs till we save the model\n",
    "    ENT_MAX = ent_max               #  Initial value for entropy\n",
    "    ENT_DECAY = ent_decay           #  Entropy decay rate\n",
    "    REW_SCALE = 25                  #  Reward scale\n",
    "    LAMBDA = 0.95                   #  Lambda, used for GAE-style advantage calculation\n",
    "    GAMMA = gamma                   #  Gamma in discounted rewards\n",
    "    \n",
    "    # We will save episodic returns for comparison later\n",
    "    returns = np.zeros(NUM_EPOCHS)\n",
    "\n",
    "    # Making runs deterministic by using specific random seeds\n",
    "    torch.random.manual_seed(123)\n",
    "    np.random.seed(123)\n",
    "    \n",
    "    # The ABR environment, the argument is the random seed\n",
    "    env = ABRSimEnv(15)\n",
    "    \n",
    "    # This is the width of the observation\n",
    "    obs_len = env.observation_space.shape[0]\n",
    "    # This is the number of possible actions\n",
    "    act_len = env.action_space.n\n",
    "\n",
    "    # Entropy factor, this is the entity pushing for exploration\n",
    "    entropy_factor = ENT_MAX\n",
    "\n",
    "    # The actor network, which we call policy_net here\n",
    "    policy_net = torch.jit.script(PensNet(obs_len, act_len, [32, 16]))\n",
    "    # The critic network, which we call value_net here\n",
    "    value_net = torch.jit.script(PensNet(obs_len, 1, [32, 16]))\n",
    "    # A buffer that takes care of storing interaction data. This replaces the 3 lists we used for the tabular policy gradient assignment\n",
    "    buff = TransitionBuffer(obs_len, env.total_num_chunks)\n",
    "\n",
    "    # Optimizers that apply gradients, SGD could be used instead but this optimizer performs better\n",
    "    net_opt_p = torch.optim.Adam(policy_net.parameters(), lr=LR, weight_decay=WD)\n",
    "    net_opt_v = torch.optim.Adam(value_net.parameters(), lr=LR, weight_decay=WD)\n",
    "    \n",
    "    # The loss for training the critic, which is basically a Mean Squared Error loss\n",
    "    net_loss = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "    # Check elapsed time\n",
    "    last_time = time.time()\n",
    "\n",
    "    # Training process\n",
    "    for epoch in trange(NUM_EPOCHS):\n",
    "\n",
    "        # Same as before, each epoch is one episode\n",
    "        obs = env.reset()\n",
    "        buff.reset_head()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # We sample an action\n",
    "            act = sample_action(policy_net, normer(obs), torch.device('cpu'))\n",
    "\n",
    "            # We take a step\n",
    "            next_obs, rew, done, info = env.step(act)\n",
    "\n",
    "            # Save our interactions in the buffer\n",
    "            buff.add_exp(obs, act, rew, next_obs, done, info['stall_time'])\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "        # The buffer size is set to the length of episodes (the video length), so this line is a sanity check\n",
    "        assert buff.buffer_full()\n",
    "\n",
    "        # Get all the saved interactions from the buffer manager\n",
    "        all_states, all_next_states, all_actions_np, all_rewards, all_dones = buff.get()\n",
    "\n",
    "        # Train A2C with GAE and entropy regularizer, the names sound scary but they are simpler than you think\n",
    "        pg_loss, v_loss, real_entropy, ret_np, v_np, adv_np = train_actor_critic(value_net, policy_net, net_opt_p, net_opt_v, net_loss, torch.device('cpu'), \n",
    "                                                                                 all_actions_np, normer(all_next_states), all_rewards / REW_SCALE, \n",
    "                                                                                 normer(all_states), all_dones, entropy_factor, GAMMA, LAMBDA)\n",
    "\n",
    "        # Normalized entropy, it ranges from 1 (fully random policy) to 0 (One action is deterministically taken)\n",
    "        norm_entropy = real_entropy / - np.log(act_len)\n",
    "        \n",
    "        # Decay the entropy factor\n",
    "        entropy_factor = max(0, entropy_factor-ENT_DECAY)\n",
    "\n",
    "        # Save the model\n",
    "        if epoch % EPOCH_SAVE == 0:\n",
    "            state_dicts = [policy_net.state_dict(), value_net.state_dict()]\n",
    "            torch.save(state_dicts, f'models/{name}/model_{epoch}')\n",
    "\n",
    "        # Update elapsed time\n",
    "        curr_time = time.time()\n",
    "        elapsed = curr_time - last_time\n",
    "        last_time = curr_time\n",
    "\n",
    "        # Log the training via tensorboard\n",
    "        log_a2c(buff, ret_np, v_np, adv_np, pg_loss, v_loss, entropy_factor, norm_entropy, elapsed, monitor, epoch)\n",
    "        \n",
    "        # Save returns\n",
    "        returns[epoch] = buff.reward_fifo.sum()\n",
    "\n",
    "    # Save the final model\n",
    "    state_dicts = [policy_net.state_dict(), value_net.state_dict()]\n",
    "    torch.save(state_dicts, f'models/{name}/model_{epoch}')\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet below runs the training process with the following config:\n",
    "\n",
    "```\n",
    "returns = train_pensieve(gamma, initial_entropy, entropy_decay_rate, tensorboard_session_name)\n",
    "```\n",
    "\n",
    "* `gamma` refers to $\\gamma$, which you have seen before in the discounted sum of rewards.\n",
    "* `initial_entropy` refers to an exploration paramater, and more accurately, its value at the start of training.\n",
    "* `entropy_decay_rate` refers to the rate that the exploration parameter decays. This will be explained more concretely later.\n",
    "* `tensorboard_session_name` refers to the name that experiment logs will be saved with tensorboard. You will soon see how to use tensoboard.\n",
    "\n",
    "`returns` refers to the sum of rewards at each epoch. This is the value we intend to maximize.\n",
    "Note that our main goal is almost always optimizing the sum of rewards, without discounting. Discounting rewards is a trick we use to simplify training procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this code snippet (it takes 2 minutes on a MBP 2019), and observe the returns of an optimized and tuned pensieve training process. Notice how at first returns are small but after training they gradually increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = train_pensieve(gamma=0.9, ent_max=1, ent_decay=1/400, name='pensieve')\n",
    "plt.figure()\n",
    "plt.plot(ewma(ret, 0.97), color='C0')          # This is called an Exponentially Weighted Moving Average. It sounds complicated but it essentially smooths out noisy curves\n",
    "plt.plot(ret, alpha=0.2, color='C0')\n",
    "plt.ylim(bottom=-1500, top=1500)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('QoE Returns');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5\n",
    "Try decreasing and increasing gamma ($\\gamma$).\n",
    "You can try the extreme ends, namely $\\gamma=1, \\gamma=0$. \n",
    "\n",
    "Plot the returns for these schemes, and compare them to the original setting of $\\gamma=0.9$.\n",
    "\n",
    "Can you explain the difference based on what $\\gamma$ does?\n",
    "\n",
    "\n",
    "Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### START OF ASSIGNMENT 5 ###################################\n",
    "############# Your code below #################\n",
    "# ret_low_gamma = ...\n",
    "# ret_high_gamma = ...\n",
    "\n",
    "# plt.plot(...)\n",
    "########################### END OF ASSIGNMENT 5 #####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 6\n",
    "You observed an exploration scheme in Q-Learning, called epsilon-greedy. There is a similar thing in Actor Critic methods.\n",
    "\n",
    "In the Advantage Actor Critic setup that you saw before, we use entropy regularization to explore. The entropy of a policy, $H(\\pi)$ refers to how random it is. We want this entropy to be high at the start of training and gradually go down, so that by the end the policies are roughly deterministic.\n",
    "\n",
    "Towards this end, the entropy of the policy is added in the policy gradient loss with some coefficient $\\kappa$:\n",
    "$$loss = J(\\theta) + \\kappa \\cdot H(\\pi_\\theta)$$\n",
    "\n",
    "This $\\kappa$ determines how important increasing exploration is compared to optimizing returns. To motivate high exploration at the beginning and low exploration by the end, $\\kappa$ is annealed from some high value to zero in the training process.\n",
    "\n",
    "To see the effect of this entropy regularization scheme, try two variants:\n",
    "* No entropy regularization at all\n",
    "* No decaying the entropy\n",
    "\n",
    "Plot the returns for these schemes, and compare them to the original setting.\n",
    "\n",
    "Can you explain the difference based on what we described here?\n",
    "\n",
    "\n",
    "Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### START OF ASSIGNMENT 6 ###################################\n",
    "############# Your code below #################\n",
    "# ret_no_entropy = ...\n",
    "# ret_no_decay = ...\n",
    "\n",
    "# plt.plot(...)\n",
    "########################### END OF ASSIGNMENT 6 #####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "Tensorboard is an automatic and live visualization tool for ML and RL training. We log many values in the course of training, such as policy gradient loss, critic training loss, average action index, average rewards, average buffer in ABR sessions, etc. You can view all these logs, from environmental ones to RL training metrics and analyze/debug the training process.\n",
    "\n",
    "To use tensorboard, go to the `tensorboard` folder and in a terminal shell, run the following command:\n",
    "```\n",
    "tensorboard --logdir=. --port= 10234\n",
    "```\n",
    "Then in a web browser, go to this url and use tensorboard:\n",
    "[localhost:10234](http://localhost:10234/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyto] *",
   "language": "python",
   "name": "conda-env-pyto-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
