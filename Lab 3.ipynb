{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 300\n",
    "\n",
    "from grid import GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid-World RL\n",
    "\n",
    "We're first going to get acquainted with the two flavors of RL we learn in class (Q-learning and Policy Gradient). To keep things simple, we'll use a Grid World environment; the same one you saw in the lectures.\n",
    "\n",
    "First, let's create an gridworld environment object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this environment looks like beyond the code. We can use the `render` method to see it.\n",
    "\n",
    "Our grid world has 4 columns (x-axis) and 3 rows (y-axis). The blue circle is the agent and it starts at $(x, y) = (0, 1)$. The \"objective\" state is the one with the coin, $(x, y) = (3, 0)$ and the death state is at $(x, y) = (3, 1)$. The reward of going to any state is $-0.05$, except the objective ($r=1$) and death states ($r=-1$).\n",
    "\n",
    "Also, there is a wall (or filled space) at $(x, y) = (1, 1)$. so you can not go to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 317.6825 248.518125\" width=\"317.6825pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2021-10-13T19:37:45.653830</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 248.518125 \n",
       "L 317.6825 248.518125 \n",
       "L 317.6825 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 20.5625 224.64 \n",
       "L 310.4825 224.64 \n",
       "L 310.4825 7.2 \n",
       "L 20.5625 7.2 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p786d7dcb47)\">\n",
       "    <image height=\"217.44\" id=\"imageb2f9bff686\" transform=\"scale(1 -1)translate(0 -217.44)\" width=\"289.92\" x=\"20.5625\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-7.2\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"m0a2d181411\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"57.1649\" xlink:href=\"#m0a2d181411\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(53.98365 239.238438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"129.6449\" xlink:href=\"#m0a2d181411\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(126.46365 239.238438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"202.1249\" xlink:href=\"#m0a2d181411\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(198.94365 239.238438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"274.6049\" xlink:href=\"#m0a2d181411\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(271.42365 239.238438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 40.578125 39.3125 \n",
       "Q 47.65625 37.796875 51.625 33 \n",
       "Q 55.609375 28.21875 55.609375 21.1875 \n",
       "Q 55.609375 10.40625 48.1875 4.484375 \n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \n",
       "Q 12.796875 0.390625 7.625 2.203125 \n",
       "L 7.625 11.71875 \n",
       "Q 11.71875 9.328125 16.59375 8.109375 \n",
       "Q 21.484375 6.890625 26.8125 6.890625 \n",
       "Q 36.078125 6.890625 40.9375 10.546875 \n",
       "Q 45.796875 14.203125 45.796875 21.1875 \n",
       "Q 45.796875 27.640625 41.28125 31.265625 \n",
       "Q 36.765625 34.90625 28.71875 34.90625 \n",
       "L 20.21875 34.90625 \n",
       "L 20.21875 43.015625 \n",
       "L 29.109375 43.015625 \n",
       "Q 36.375 43.015625 40.234375 45.921875 \n",
       "Q 44.09375 48.828125 44.09375 54.296875 \n",
       "Q 44.09375 59.90625 40.109375 62.90625 \n",
       "Q 36.140625 65.921875 28.71875 65.921875 \n",
       "Q 24.65625 65.921875 20.015625 65.03125 \n",
       "Q 15.375 64.15625 9.8125 62.3125 \n",
       "L 9.8125 71.09375 \n",
       "Q 15.4375 72.65625 20.34375 73.4375 \n",
       "Q 25.25 74.21875 29.59375 74.21875 \n",
       "Q 40.828125 74.21875 47.359375 69.109375 \n",
       "Q 53.90625 64.015625 53.90625 55.328125 \n",
       "Q 53.90625 49.265625 50.4375 45.09375 \n",
       "Q 46.96875 40.921875 40.578125 39.3125 \n",
       "z\n",
       "\" id=\"DejaVuSans-51\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"m3d43b03c99\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m3d43b03c99\" y=\"43.8024\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(7.2 47.601619)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m3d43b03c99\" y=\"116.2824\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(7.2 120.081619)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m3d43b03c99\" y=\"188.7624\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(7.2 192.561619)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\">\n",
       "    <path clip-path=\"url(#p786d7dcb47)\" d=\"M 93.4049 224.64 \n",
       "L 93.4049 7.2 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n",
       "    <path clip-path=\"url(#p786d7dcb47)\" d=\"M 165.8849 224.64 \n",
       "L 165.8849 7.2 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n",
       "    <path clip-path=\"url(#p786d7dcb47)\" d=\"M 238.3649 224.64 \n",
       "L 238.3649 7.2 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_2\">\n",
       "    <path clip-path=\"url(#p786d7dcb47)\" d=\"M 20.5625 80.0424 \n",
       "L 310.4825 80.0424 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n",
       "    <path clip-path=\"url(#p786d7dcb47)\" d=\"M 20.5625 152.5224 \n",
       "L 310.4825 152.5224 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 20.5625 224.64 \n",
       "L 20.5625 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 310.4825 224.64 \n",
       "L 310.4825 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 20.5625 224.64 \n",
       "L 310.4825 224.64 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 20.5625 7.2 \n",
       "L 310.4825 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <image height=\"24\" id=\"imageda67794b27\" transform=\"scale(1 -1)translate(0 -24)\" width=\"24\" x=\"280.7249\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-13.6824\"/>\n",
       "   <image height=\"29.28\" id=\"imageb39400b2fe\" transform=\"scale(1 -1)translate(0 -29.28)\" width=\"24.72\" x=\"276.8009\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-87.0564\"/>\n",
       "   <g id=\"patch_7\">\n",
       "    <path clip-path=\"url(#p786d7dcb47)\" d=\"M 57.1649 98.1624 \n",
       "C 61.97038 98.1624 66.579687 100.071638 69.977675 103.469625 \n",
       "C 73.375662 106.867613 75.2849 111.47692 75.2849 116.2824 \n",
       "C 75.2849 121.08788 73.375662 125.697187 69.977675 129.095175 \n",
       "C 66.579687 132.493162 61.97038 134.4024 57.1649 134.4024 \n",
       "C 52.35942 134.4024 47.750113 132.493162 44.352125 129.095175 \n",
       "C 40.954138 125.697187 39.0449 121.08788 39.0449 116.2824 \n",
       "C 39.0449 111.47692 40.954138 106.867613 44.352125 103.469625 \n",
       "C 47.750113 100.071638 52.35942 98.1624 57.1649 98.1624 \n",
       "z\n",
       "\" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p786d7dcb47\">\n",
       "   <rect height=\"217.44\" width=\"289.92\" x=\"20.5625\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 1800x1200 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.render();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to map our action to numbers to work with them. There are 4 actions in this environment: Right ($a=0$), Down ($a=1$), Left ($a=2$), Up ($a=3$). If we go in a direction that is blocked (hitting a wall), we don't move.\n",
    "\n",
    "Try running the script below, which goes through a movement sequence and visulizes it. We can give actions to the environment through the `step` method.\n",
    "\n",
    "Everytime you want to start from the begining, you have to use the `reset` method to start over.\n",
    "\n",
    "Note: The methods that we are using here conform to the standard Atari Gym API (from the DQN paper). This API has become best practice for working with RL environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 317.6825 262.19625\" width=\"317.6825pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2021-10-13T19:52:39.609980</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 262.19625 \n",
       "L 317.6825 262.19625 \n",
       "L 317.6825 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 20.5625 224.64 \n",
       "L 310.4825 224.64 \n",
       "L 310.4825 7.2 \n",
       "L 20.5625 7.2 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pf5b2b587eb)\">\n",
       "    <image height=\"217.44\" id=\"imagef39708f054\" transform=\"scale(1 -1)translate(0 -217.44)\" width=\"289.92\" x=\"20.5625\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-7.2\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"m4162281924\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"57.1649\" xlink:href=\"#m4162281924\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(53.98365 239.238438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"129.6449\" xlink:href=\"#m4162281924\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(126.46365 239.238438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"202.1249\" xlink:href=\"#m4162281924\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(198.94365 239.238438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"274.6049\" xlink:href=\"#m4162281924\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(271.42365 239.238438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 40.578125 39.3125 \n",
       "Q 47.65625 37.796875 51.625 33 \n",
       "Q 55.609375 28.21875 55.609375 21.1875 \n",
       "Q 55.609375 10.40625 48.1875 4.484375 \n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \n",
       "Q 12.796875 0.390625 7.625 2.203125 \n",
       "L 7.625 11.71875 \n",
       "Q 11.71875 9.328125 16.59375 8.109375 \n",
       "Q 21.484375 6.890625 26.8125 6.890625 \n",
       "Q 36.078125 6.890625 40.9375 10.546875 \n",
       "Q 45.796875 14.203125 45.796875 21.1875 \n",
       "Q 45.796875 27.640625 41.28125 31.265625 \n",
       "Q 36.765625 34.90625 28.71875 34.90625 \n",
       "L 20.21875 34.90625 \n",
       "L 20.21875 43.015625 \n",
       "L 29.109375 43.015625 \n",
       "Q 36.375 43.015625 40.234375 45.921875 \n",
       "Q 44.09375 48.828125 44.09375 54.296875 \n",
       "Q 44.09375 59.90625 40.109375 62.90625 \n",
       "Q 36.140625 65.921875 28.71875 65.921875 \n",
       "Q 24.65625 65.921875 20.015625 65.03125 \n",
       "Q 15.375 64.15625 9.8125 62.3125 \n",
       "L 9.8125 71.09375 \n",
       "Q 15.4375 72.65625 20.34375 73.4375 \n",
       "Q 25.25 74.21875 29.59375 74.21875 \n",
       "Q 40.828125 74.21875 47.359375 69.109375 \n",
       "Q 53.90625 64.015625 53.90625 55.328125 \n",
       "Q 53.90625 49.265625 50.4375 45.09375 \n",
       "Q 46.96875 40.921875 40.578125 39.3125 \n",
       "z\n",
       "\" id=\"DejaVuSans-51\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_5\">\n",
       "     <!-- Done!!! -->\n",
       "     <g transform=\"translate(146.35375 252.916563)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path d=\"M 19.671875 64.796875 \n",
       "L 19.671875 8.109375 \n",
       "L 31.59375 8.109375 \n",
       "Q 46.6875 8.109375 53.6875 14.9375 \n",
       "Q 60.6875 21.78125 60.6875 36.53125 \n",
       "Q 60.6875 51.171875 53.6875 57.984375 \n",
       "Q 46.6875 64.796875 31.59375 64.796875 \n",
       "z\n",
       "M 9.8125 72.90625 \n",
       "L 30.078125 72.90625 \n",
       "Q 51.265625 72.90625 61.171875 64.09375 \n",
       "Q 71.09375 55.28125 71.09375 36.53125 \n",
       "Q 71.09375 17.671875 61.125 8.828125 \n",
       "Q 51.171875 0 30.078125 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-68\"/>\n",
       "       <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "       <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-110\"/>\n",
       "       <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "\" id=\"DejaVuSans-101\"/>\n",
       "       <path d=\"M 15.09375 12.40625 \n",
       "L 25 12.40625 \n",
       "L 25 0 \n",
       "L 15.09375 0 \n",
       "z\n",
       "M 15.09375 72.90625 \n",
       "L 25 72.90625 \n",
       "L 25 40.921875 \n",
       "L 24.03125 23.484375 \n",
       "L 16.109375 23.484375 \n",
       "L 15.09375 40.921875 \n",
       "z\n",
       "\" id=\"DejaVuSans-33\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-68\"/>\n",
       "      <use x=\"77.001953\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"138.183594\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "      <use x=\"201.5625\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"263.085938\" xlink:href=\"#DejaVuSans-33\"/>\n",
       "      <use x=\"303.173828\" xlink:href=\"#DejaVuSans-33\"/>\n",
       "      <use x=\"343.261719\" xlink:href=\"#DejaVuSans-33\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"m836aefa004\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m836aefa004\" y=\"43.8024\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(7.2 47.601619)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m836aefa004\" y=\"116.2824\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(7.2 120.081619)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m836aefa004\" y=\"188.7624\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(7.2 192.561619)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\">\n",
       "    <path clip-path=\"url(#pf5b2b587eb)\" d=\"M 93.4049 224.64 \n",
       "L 93.4049 7.2 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n",
       "    <path clip-path=\"url(#pf5b2b587eb)\" d=\"M 165.8849 224.64 \n",
       "L 165.8849 7.2 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n",
       "    <path clip-path=\"url(#pf5b2b587eb)\" d=\"M 238.3649 224.64 \n",
       "L 238.3649 7.2 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_2\">\n",
       "    <path clip-path=\"url(#pf5b2b587eb)\" d=\"M 20.5625 80.0424 \n",
       "L 310.4825 80.0424 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n",
       "    <path clip-path=\"url(#pf5b2b587eb)\" d=\"M 20.5625 152.5224 \n",
       "L 310.4825 152.5224 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 20.5625 224.64 \n",
       "L 20.5625 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 310.4825 224.64 \n",
       "L 310.4825 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 20.5625 224.64 \n",
       "L 310.4825 224.64 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 20.5625 7.2 \n",
       "L 310.4825 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <image height=\"24\" id=\"image3f43127e6f\" transform=\"scale(1 -1)translate(0 -24)\" width=\"24\" x=\"280.7249\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-13.6824\"/>\n",
       "   <image height=\"29.28\" id=\"image4e4476f68f\" transform=\"scale(1 -1)translate(0 -29.28)\" width=\"24.72\" x=\"276.8009\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-87.0564\"/>\n",
       "   <g id=\"patch_7\">\n",
       "    <path clip-path=\"url(#pf5b2b587eb)\" d=\"M 274.6049 98.1624 \n",
       "C 279.41038 98.1624 284.019687 100.071638 287.417675 103.469625 \n",
       "C 290.815662 106.867613 292.7249 111.47692 292.7249 116.2824 \n",
       "C 292.7249 121.08788 290.815662 125.697187 287.417675 129.095175 \n",
       "C 284.019687 132.493162 279.41038 134.4024 274.6049 134.4024 \n",
       "C 269.79942 134.4024 265.190113 132.493162 261.792125 129.095175 \n",
       "C 258.394138 125.697187 256.4849 121.08788 256.4849 116.2824 \n",
       "C 256.4849 111.47692 258.394138 106.867613 261.792125 103.469625 \n",
       "C 265.190113 100.071638 269.79942 98.1624 274.6049 98.1624 \n",
       "z\n",
       "\" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pf5b2b587eb\">\n",
       "   <rect height=\"217.44\" width=\"289.92\" x=\"20.5625\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 1800x1200 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset environment, get the initial state\n",
    "# The state in grid world is two dimenional: [x, y], where x shows the column and y the row, counting from the top left\n",
    "state = env.reset()\n",
    "\n",
    "# Map action indices to text\n",
    "direction = {0: 'right', 1: 'down', 2: 'left', 3: 'up'}\n",
    "plt.figure()\n",
    "\n",
    "# Go through a sequence of actions\n",
    "for t, action in enumerate([3, 0, 0, 1, 1, 2, 2, 0, 0, 0, 3]):\n",
    "    \n",
    "    # Take a step by choosing an action. You get 4 outputs from this:\n",
    "    #      next_state: The next state that taking the action leads to.\n",
    "    #      rew: The reward we got from taking the action\n",
    "    #      done: Is the episode over? In this environment an episode is over when we reach the objective or death states\n",
    "    #      info: Other info of interest\n",
    "    next_state, rew, done, info = env.step(action)\n",
    "    \n",
    "    # Render the environment\n",
    "    env.render(plt.gca())\n",
    "    plt.xlabel(f'Step {t}: went {direction[action]}, is in state (x, y) = ({next_state[0]}, {next_state[1]}), got a reward {rew}')\n",
    "    \n",
    "    # Some Jupyter notebook nonsense to show plots in a loop\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    time.sleep(1.0)\n",
    "\n",
    "time.sleep(3)\n",
    "plt.xlabel('Done!!!')\n",
    "display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q-Learning\n",
    "\n",
    "In this first assignment, we are going to understand and implement Q-learning in our simple grid world. Remember that Q-learning works by applying a greedy update over the current Q-values:\n",
    "\n",
    "$$Q(s, a) = \\mathbb{E}_{(r, s')~\\sim~\\epsilon(\\cdot|s, a)} \\left[ r + \\gamma \\cdot \\max_{a' \\in \\mathcal{A}}  Q(s', a') \\right]$$\n",
    "\n",
    "If this at all does not seem reasonable or familiar, you are urged to go through the lecture slides and get a clear understanding of this formula. You will need this understanding as you are going to implement it below.\n",
    "\n",
    "\n",
    "The training process for Q-learning is as follows:\n",
    "1. Take an action according to an eps-greedy policy; meaning that with a probability $\\epsilon$, take a random policy and otherwise choose the action that has the highest Q-value in that state.\n",
    "2. Use the greedy update rule to update your Q-value table\n",
    "3. Over time, anneal $\\epsilon$. Start with fully random choices ($\\epsilon=1$) at the start of training and decay it overtime to some low value, like 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "Complete the two code snippets below and get a working Q-Learning training process. The policy (visualized by arrows) should be optimal by the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Some hyperparameters used in the training process\n",
    "# DO NOT CHANGE\n",
    "NUM_EPOCHS = 10                    #  How many iterations (or epochs) will we train\n",
    "INITIAL_RANDOM_EPOCHS = 5          #  For the eps-greedy exploration process, how many initial epochs will we be completely random.\n",
    "                                   #  People usually do this to bootstrap their training from an initial random policy\n",
    "REPORT_EPOCHS = 2                  #  Visualize the policy after this many epochs\n",
    "MIN_EPS = 0.1                      #  Anneal epsilon from epsilon greedy to this value. Basically, this means that by the end we will alway take 10% of actions randomly\n",
    "GAMMA = 0.9                        #  This is the gamma you have seen many times with rewards and returns\n",
    "\n",
    "# Initialize a Q-value table. You can try initializing it from a random starting point, and see for yourself that it works no matter what values you start with.\n",
    "qtable = np.zeros((4, 3, 4))\n",
    "# This is the schedule for epsilon. It seems very complicated but it is simply a linear function that is clipped to a range of 0.1 and 1\n",
    "eps = lambda step: np.clip(NUM_EPOCHS/(NUM_EPOCHS-INITIAL_RANDOM_EPOCHS)*(1-step/NUM_EPOCHS), a_max=1, a_min=MIN_EPS)\n",
    "# This is the learning rate we use. Remember that learning rate has some special conditions on it for Tabular Q-learning. \n",
    "# We advise you to look over the lecture slides amd figure out why this learning rate function is a good choice\n",
    "lr = lambda step: 1/(step+1)\n",
    "\n",
    "# A random number generator. We use this to make our experiments reproducible. This is very good practice in RL training.\n",
    "rng = np.random.RandomState(seed=3)\n",
    "\n",
    "env = GridWorld()\n",
    "\n",
    "for epoch in trange(NUM_EPOCHS):\n",
    "    # Each epoch is one full episode\n",
    "    # An episode starts from the initial state (hence the reset function) and ends when we reach a terminal state, like the objective or death state.\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # We continue an episode until it terminates\n",
    "    while not done:\n",
    "        \n",
    "        ########################### START OF ASSIGNMENT 1 ###################################\n",
    "        ############# Your code below #################\n",
    "        # Write the following code to choose the action. Remember that the policy is epsilon-greedy.\n",
    "        # Use rng for random numbers\n",
    "        \n",
    "        # act = ...\n",
    "        \n",
    "        ########################### END OF ASSIGNMENT 1 #####################################\n",
    "        \n",
    "        next_obs, rew, done, info = env.step(act)\n",
    "        \n",
    "        alpha = lr(epoch)\n",
    "        ########################### START OF ASSIGNMENT 1 ###################################\n",
    "        ############# Your code below #################\n",
    "        # Write the following code to apply the greedy Q-learning update\n",
    "        # Use alpha as the learning rate for this step\n",
    "        \n",
    "        # qtable = ...\n",
    "        \n",
    "        ########################### END OF ASSIGNMENT 1 #####################################\n",
    "        \n",
    "        obs = next_obs\n",
    "    \n",
    "    # Visualize the policy based on the Q-tables\n",
    "    if epoch % REPORT_EPOCHS == 0:\n",
    "        ax = env.visualize_qtable(qtable)\n",
    "        ax.set_title(f'Epoch {epoch}')\n",
    "\n",
    "# Visualize the final policy\n",
    "ax = env.visualize_qtable(qtable)\n",
    "ax.set_title(f'Final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Policy Gradient\n",
    "\n",
    "In the second assignment, we are going to use policy gradient instead. Policy gradient worked by use some policy, characterized by $\\theta$, to sample trajectories. The value of those trajectories look like this:\n",
    "\n",
    "$$J(\\theta)=\\mathbb{E}_{\\tau \\sim p(\\tau;\\theta)} \\left[ R(\\tau) \\right] = \\sum_{\\tau} p(\\tau; \\theta) R(\\tau)$$\n",
    "\n",
    "After defining this function, policy gradient improves the returns by moving the parameters $\\theta$ in the direction of $\\nabla_\\theta J(\\theta)$. This is called gradient ascent. The policy gradient is:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta)=\\mathbb{E}_{\\tau \\sim p(\\tau;\\theta)} \\left[ \\nabla_\\theta \\log p(\\tau; \\theta))~R(\\tau) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulating the policy\n",
    "\n",
    "Now suppose we wanted to try this. How would we define a policy? We'll create one the same way we create a Q-value table. We'll create a policy table, where each state and action have a weight. These weights are basically our parameters, $\\theta$. So in the grid world example, for each state there are 4 numbers for 4 directions. How do we define a policy using these numbers?\n",
    "\n",
    "Say for state $s$ the numbers are $v_1(s), v_2(s), v_3(s), ..., v_M(s)$, where the number of actions are $M=|\\mathcal{A}|$. We could say the possibility of action $i$ is proportional to $v_i(s)$:\n",
    "\n",
    "$$p(a=i|s)=\\pi(i|s)=\\frac{v_i(s)}{\\sum_{a=1}^M v_a(s)}$$\n",
    "\n",
    "What is the issue with this? It restricts all $v_a(s)$ to be positive and non-zero. This is not favorable. If the policy gradient ascent causes one of them to become negative, a probability becomes negative and the whole setup fails. \n",
    "\n",
    "We'll make a small change that fixes this. We'll have the possibility of action $i$ be proportional to $e^{v_i(s)}$. This avoids the issues from before and looks like this:\n",
    "\n",
    "$$p(a=i|s)=\\pi(i|s)=\\frac{e^{v_i(s)}}{\\sum_{a=1}^M e^{v_a(s)}}$$\n",
    "\n",
    "This is called a Softmax function and is very popular in Machine Learning. The best point about all of this is that the gradient is very simple. Basically, the partial derivative of $p(a=i|s)$ with respect to any $v$ is:\n",
    "\n",
    "$$\\frac{\\partial p(a=i|s)}{\\partial v_{\\hat{a}}(\\hat{s})} = 0,~~~~\\text{If }\\hat{s} \\neq s$$\n",
    "Since the policy in state s only depends on values that define the policy for state s. We also have:\n",
    "$$\\frac{\\partial p(a=i|s)}{\\partial v_{\\hat{a}}(\\hat{s})} = -p(a=\\hat{a}|s),~~~~\\text{If }\\hat{s} = s ~ \\text{and } \\hat{a} \\neq i$$\n",
    "$$\\frac{\\partial p(a=i|s)}{\\partial v_{\\hat{a}}(\\hat{s})} = 1-p(a=i|s),~~~~\\text{If }\\hat{s} = s ~ \\text{and } \\hat{a} = i$$\n",
    "You are encouraged to try the derivation yourself, to gain some intuition.\n",
    "\n",
    "Note: We are omitting a few things from the standard Softmax function for simplicity. You can research the general version yourself, but this explanation should suffice for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Softmax function\n",
    "# This function takes a one-dimensional array x, and outputs the softmax probabilities.\n",
    "def softmax(x):\n",
    "    max_x = np.max(x, axis=-1, keepdims=True)\n",
    "    e_x = np.exp(x - max_x)\n",
    "    sum_x = np.sum(e_x, axis=-1, keepdims=True)\n",
    "    return e_x / sum_x\n",
    "\n",
    "# The gradient of the log of the Softmax function\n",
    "# This function takes a one-dimensional array x, and outputs a gradient vector for the parameters (x).\n",
    "def nabla_log_softmax(x, index):\n",
    "    s = -softmax(x)\n",
    "    s[index] += 1\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "Now that we have a policy, let's go over the the training process for policy gradient:\n",
    "1. Take actions according to your policy table.\n",
    "2. Once you have a full episode $\\tau$, use the policy gradient by:\n",
    "    1. Calculate the episode return $R(\\tau)$.\n",
    "    2. Calculate the gradient of the log probability $\\nabla_\\theta \\log p(\\tau; \\theta))$, for the episode. You have to loop over all actions and calculate the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "Complete the code snippet below and get a working policy-gradient training process. The policy is visualized by arrows in 4 directions, where the length of each arrow shows how likely it is.\n",
    "The policy should be somewhat optimal by the end in the top row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Some hyperparameters used in the training process\n",
    "# DO NOT CHANGE\n",
    "NUM_EPOCHS = 150                   #  How many iterations (or epochs) will we train\n",
    "REPORT_EPOCHS = 50                 #  Visualize the policy after this many epochs\n",
    "GAMMA = 0.9                        #  This is the gamma you have seen many times with rewards and returns\n",
    "\n",
    "\n",
    "# Initialize a policy table. Unlike Q-Learning, it is imperative to start with a fully zero table, as that means the starting policy is random (Why?).\n",
    "# Without this, we would not explore.\n",
    "policy_table = np.zeros((4, 3, 4))\n",
    "# This is the learning rate we use. Unlike Q-Learning, the learning rate here is fixed\n",
    "lr = 1e-1\n",
    "\n",
    "# A random number generator. We use this to make our experiments reproducible. This is very good practice in RL training.\n",
    "rng = np.random.RandomState(seed=3)\n",
    "\n",
    "env = GridWorld()\n",
    "\n",
    "for epoch in trange(NUM_EPOCHS):\n",
    "    # Each epoch is one full episode\n",
    "    # An episode starts from the initial state (hence the reset function) and ends when we reach a terminal state, like the objective or death state.\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # We have to store our interactions with the environment, so we can apply the policy gradient update afterwards. Why didn't we need this in Q-Learning?\n",
    "    list_obs = []\n",
    "    list_rew = []\n",
    "    list_act = []\n",
    "    \n",
    "    # We continue an episode until it terminates\n",
    "    while not done:\n",
    "        # Choose the next action. \"softmax(policy_table[obs[0], obs[1]])\" gives a distribution over actions.\n",
    "        # \"rng.choice\" can sample actions with that distribution\n",
    "        act = rng.choice(4, p=softmax(policy_table[obs[0], obs[1]]))\n",
    "        \n",
    "        next_obs, rew, done, info = env.step(act)\n",
    "        \n",
    "        # Store interactions\n",
    "        list_obs.append(obs)\n",
    "        list_rew.append(rew)\n",
    "        list_act.append(act)\n",
    "        \n",
    "        obs = next_obs\n",
    "    \n",
    "    ########################### START OF ASSIGNMENT 2 ###################################\n",
    "    ############# Your code below #################\n",
    "    # Write the following code to apply the policy-gradient update. We have already laid out the steps.\n",
    "    # 1. You have to compute R(tau) and save it to rew_all\n",
    "    # 2. You have to compute the gradients of log softmax policies\n",
    "    # 3. You have to apply the gradient ascent based on these two\n",
    "\n",
    "    # rew_all = ...\n",
    "\n",
    "    nabla_policy_table = np.zeros((4, 3, 4))\n",
    "\n",
    "    # for obs, act in zip(list_obs, list_act):\n",
    "    #     nabla_policy_table += ...\n",
    "\n",
    "    # policy_table += lr * ...\n",
    "\n",
    "    ########################### END OF ASSIGNMENT 2 #####################################\n",
    "    \n",
    "    # Visualize the policy\n",
    "    if epoch % REPORT_EPOCHS == 0:\n",
    "        ax = env.visualize_policy_table(softmax(policy_table))\n",
    "        ax.set_title(f'Epoch {epoch}')\n",
    "        \n",
    "\n",
    "# Visualize the final policy\n",
    "ax = env.visualize_policy_table(softmax(policy_table))\n",
    "ax.set_title(f'Final')\n",
    "\n",
    "policy_table_1 = policy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An improvement\n",
    "Remember that we could simplify the policy gradient formula by scoring an action only based on the actions that come after it. Basically, a rederivation of the of the previous policy gradient looked like this:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta)= \\mathbb{E} \\left[ \\sum_{t\\ge 0} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot (\\sum_{l\\ge0}\\gamma^l r_l) \\right]$$\n",
    "\n",
    "The rewards we assign to action $a_t$ in step $s_t$ include rewards $r_0, r_1, ... r_{t-1}$. However, $a_t$ had nothing to do with previous rewards and is only responsible for the returns after $t$. So we could make a small change in the formula:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta)= \\mathbb{E} \\left[ \\sum_{t\\ge 0} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot (\\sum_{l\\ge t}\\gamma^l r_l) \\right]$$\n",
    "\n",
    "By employing this trick, we decrease the variance of the policy gradient method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3\n",
    "Complete the code snippet below and decrease the policy gradient variance. We generally expect the policy to converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Some hyperparameters used in the training process\n",
    "# DO NOT CHANGE\n",
    "NUM_EPOCHS = 150                   #  How many iterations (or epochs) will we train\n",
    "REPORT_EPOCHS = 50                 #  Visualize the policy after this many epochs\n",
    "GAMMA = 0.9                        #  This is the gamma you have seen many times with rewards and returns\n",
    "\n",
    "\n",
    "# Initialize a policy table. Unlike Q-Learning, it is imperative to start with a fully zero table, as that means the starting policy is random (Why?).\n",
    "# Without this, we would not explore.\n",
    "policy_table = np.zeros((4, 3, 4))\n",
    "# This is the learning rate we use. Unlike Q-Learning, the learning rate here is fixed\n",
    "lr = 1e-1\n",
    "\n",
    "# A random number generator. We use this to make our experiments reproducible. This is very good practice in RL training.\n",
    "rng = np.random.RandomState(seed=3)\n",
    "\n",
    "env = GridWorld()\n",
    "\n",
    "for epoch in trange(NUM_EPOCHS):\n",
    "    # Each epoch is one full episode\n",
    "    # An episode starts from the initial state (hence the reset function) and ends when we reach a terminal state, like the objective or death state.\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # We have to store our interactions with the environment, so we can apply the policy gradient update afterwards. Why didn't we need this in Q-Learning?\n",
    "    list_obs = []\n",
    "    list_rew = []\n",
    "    list_act = []\n",
    "    \n",
    "    # We continue an episode until it terminates\n",
    "    while not done:\n",
    "        # Choose the next action. \"softmax(policy_table[obs[0], obs[1]])\" gives a distribution over actions.\n",
    "        # \"rng.choice\" can sample actions with that distribution\n",
    "        act = rng.choice(4, p=softmax(policy_table[obs[0], obs[1]]))\n",
    "        \n",
    "        next_obs, rew, done, info = env.step(act)\n",
    "        \n",
    "        # Store interactions\n",
    "        list_obs.append(obs)\n",
    "        list_rew.append(rew)\n",
    "        list_act.append(act)\n",
    "        \n",
    "        obs = next_obs\n",
    "    \n",
    "    ########################### START OF ASSIGNMENT 3 ###################################\n",
    "    ############# Your code below #################\n",
    "    # Write the following code to apply the policy-gradient update. We have already laid out the steps.\n",
    "    # 1. You have to compute the [sum of gamma^l r_l for l >= t] and save it to returns\n",
    "    # 2. You have to compute the gradients of log softmax policies times the returns from step 1 (this is different from before, why?)\n",
    "    # 3. You have to apply the gradient ascent based on step 2\n",
    "\n",
    "    traj_length = len(rew_list)\n",
    "    returns = np.zeros(traj_length)\n",
    "    nabla_policy_table = np.zeros((4, 3, 4))\n",
    "    \n",
    "    # for i in range(traj_length):\n",
    "    #     returns[i] = ...\n",
    "\n",
    "    # for obs, act in zip(list_obs, list_act):\n",
    "    #     nabla_policy_table += ...\n",
    "\n",
    "    # policy_table += lr * ...\n",
    "\n",
    "    ########################### END OF ASSIGNMENT 3 #####################################\n",
    "    \n",
    "    # Visualize the policy\n",
    "    if epoch % REPORT_EPOCHS == 0:\n",
    "        ax = env.visualize_policy_table(softmax(policy_table))\n",
    "        ax.set_title(f'Epoch {epoch}')\n",
    "        \n",
    "\n",
    "# Visualize the final policy\n",
    "ax = env.visualize_policy_table(softmax(policy_table))\n",
    "ax.set_title(f'Final')\n",
    "\n",
    "policy_table_1 = policy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the result of Assignment 2 and Assignment 3. The policy at the end of Assignment 3 should have converged faster than 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 2, figsize=(12.8, 4.8))\n",
    "env.visualize_policy_table(softmax(policy_table_1), axes[0])\n",
    "axes[0].set_title(f'Final policy in Assignment 2')\n",
    "env.visualize_policy_table(softmax(policy_table_2), axes[1])\n",
    "axes[1].set_title(f'Final policy in Assignment 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "You might have noticed, in Assignment 2 and 3, that the policies for the bottom row don't look so great. Why is this? Does it mean we are getting bad returns?\n",
    "\n",
    "\n",
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Bit Rate (ABR) and Pensieve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last part of this lab, we'll recreate [Pensieve](https://web.mit.edu/pensieve/)!!!\n",
    "\n",
    "The code snippet below runs a training process for the ABR environment (slightly simplified to converge in 750 epochs). We use Pytorch to handle neural networks and automatic gradient calculation.\n",
    "\n",
    "It is beneficial, but not obligatory, to go through this code and understand what is happening. If you are interested, you could also look at `core_pg.py` to see how the policy gradient variant in this setup works. \n",
    "\n",
    "Just briefly introducing this environment:\n",
    "\n",
    "* The point is to download video chunks in a streaming session, while optimizing for high quality video and low rebuffering.\n",
    "* The state in the ABR environment is:\n",
    "    * the current buffer occupancy\n",
    "    * previous action\n",
    "    * chunks left in the video\n",
    "    * some history of throughput and download times\n",
    "    * chunk sizes for the next video chunk\n",
    "* The actions in this environment are a set of video encodings, each different in size and quality. Naturally, a higher size means a higher quality.\n",
    "* The reward is some QoE (Quality of Experience) metric. This metric is a linear combination of quality and rebuffering penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core_pg import sample_action, train_actor_critic\n",
    "from core_log import log_a2c\n",
    "from abr_env.abr import ABRSimEnv\n",
    "from nn import PensNet\n",
    "from utils import ewma\n",
    "from buffer import TransitionBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from normer import Normer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "def train_pensieve(gamma, ent_max, ent_decay, name):\n",
    "    # Create some folders to save models and tensorboard data\n",
    "    os.makedirs(f'models/{name}/', exist_ok=True)\n",
    "    os.makedirs('tensorboard/', exist_ok=True)\n",
    "\n",
    "    # Monitor training with tensorboard\n",
    "    monitor = SummaryWriter(f'tensorboard/{name}', flush_secs=10)\n",
    "    \n",
    "    # An object to normalize the observation vectors, This usually speeds up training.\n",
    "    normer = Normer()\n",
    "\n",
    "    # A set of hyperparameters\n",
    "    # DO NOT CHANGE\n",
    "    LR = 1e-2                       #  Learning rate\n",
    "    WD = 1e-4                       #  Weight decay (or in other words, L2 regularization penalty)\n",
    "    NUM_EPOCHS = 750                #  How many epochs to train\n",
    "    EPOCH_SAVE = 100                #  How many epochs till we save the model\n",
    "    ENT_MAX = ent_max               #  Initial value for entropy\n",
    "    ENT_DECAY = ent_decay           #  Entropy decay rate\n",
    "    REW_SCALE = 25                  #  Reward scale\n",
    "    LAMBDA = 0.95                   #  Lambda, used for GAE-style advantage calculation\n",
    "    GAMMA = gamma                   #  Gamma in discounted rewards\n",
    "    \n",
    "    # We will save episodic returns for comparison later\n",
    "    returns = np.zeros(NUM_EPOCHS)\n",
    "\n",
    "    # Making runs deterministic by using specific random seeds\n",
    "    torch.random.manual_seed(123)\n",
    "    np.random.seed(123)\n",
    "    \n",
    "    # The ABR environment, the argument is the random seed\n",
    "    env = ABRSimEnv(15)\n",
    "    \n",
    "    # This is the width of the observation\n",
    "    obs_len = env.observation_space.shape[0]\n",
    "    # This is the number of possible actions\n",
    "    act_len = env.action_space.n\n",
    "\n",
    "    # Entropy factor, this is the entity pushing for exploration\n",
    "    entropy_factor = ENT_MAX\n",
    "\n",
    "    # The actor network, which we call policy_net here\n",
    "    policy_net = torch.jit.script(PensNet(obs_len, act_len, [32, 16]))\n",
    "    # The critic network, which we call value_net here\n",
    "    value_net = torch.jit.script(PensNet(obs_len, 1, [32, 16]))\n",
    "    # A buffer that takes care of storing interaction data. This replaces the 3 lists we used for the tabular policy gradient assignment\n",
    "    buff = TransitionBuffer(obs_len, env.total_num_chunks)\n",
    "\n",
    "    # Optimizers that apply gradients, SGD could be used instead but this optimizer performs better\n",
    "    net_opt_p = torch.optim.Adam(policy_net.parameters(), lr=LR, weight_decay=WD)\n",
    "    net_opt_v = torch.optim.Adam(value_net.parameters(), lr=LR, weight_decay=WD)\n",
    "    \n",
    "    # The loss for training the critic, which is basically a Mean Squared Error loss\n",
    "    net_loss = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "    # Check elapsed time\n",
    "    last_time = time.time()\n",
    "\n",
    "    # Training process\n",
    "    for epoch in trange(NUM_EPOCHS):\n",
    "\n",
    "        # Same as before, each epoch is one episode\n",
    "        obs = env.reset()\n",
    "        buff.reset_head()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # We sample an action\n",
    "            act = sample_action(policy_net, normer(obs), torch.device('cpu'))\n",
    "\n",
    "            # We take a step\n",
    "            next_obs, rew, done, info = env.step(act)\n",
    "\n",
    "            # Save our interactions in the buffer\n",
    "            buff.add_exp(obs, act, rew, next_obs, done, info['stall_time'])\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "        # The buffer size is set to the length of episodes (the video length), so this line is a sanity check\n",
    "        assert buff.buffer_full()\n",
    "\n",
    "        # Get all the saved interactions from the buffer manager\n",
    "        all_states, all_next_states, all_actions_np, all_rewards, all_dones = buff.get()\n",
    "\n",
    "        # Train A2C with GAE and entropy regularizer, the names sound scary but they are simpler than you think\n",
    "        pg_loss, v_loss, real_entropy, ret_np, v_np, adv_np = train_actor_critic(value_net, policy_net, net_opt_p, net_opt_v, net_loss, torch.device('cpu'), \n",
    "                                                                                 all_actions_np, normer(all_next_states), all_rewards / REW_SCALE, \n",
    "                                                                                 normer(all_states), all_dones, entropy_factor, GAMMA, LAMBDA)\n",
    "\n",
    "        # Normalized entropy, it ranges from 1 (fully random policy) to 0 (One action is deterministically taken)\n",
    "        norm_entropy = real_entropy / - np.log(act_len)\n",
    "        \n",
    "        # Decay the entropy factor\n",
    "        entropy_factor = max(0, entropy_factor-ENT_DECAY)\n",
    "\n",
    "        # Save the model\n",
    "        if epoch % EPOCH_SAVE == 0:\n",
    "            state_dicts = [policy_net.state_dict(), value_net.state_dict()]\n",
    "            torch.save(state_dicts, f'models/{name}/model_{epoch}')\n",
    "\n",
    "        # Update elapsed time\n",
    "        curr_time = time.time()\n",
    "        elapsed = curr_time - last_time\n",
    "        last_time = curr_time\n",
    "\n",
    "        # Log the training via tensorboard\n",
    "        log_a2c(buff, ret_np, v_np, adv_np, pg_loss, v_loss, entropy_factor, norm_entropy, elapsed, monitor, epoch)\n",
    "        \n",
    "        # Save returns\n",
    "        returns[epoch] = buff.reward_fifo.sum()\n",
    "\n",
    "    # Save the final model\n",
    "    state_dicts = [policy_net.state_dict(), value_net.state_dict()]\n",
    "    torch.save(state_dicts, f'models/{name}/model_{epoch}')\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet below runs the training process with the following config:\n",
    "\n",
    "```\n",
    "returns = train_pensieve(gamma, initial_entropy, entropy_decay_rate, tensorboard_session_name)\n",
    "```\n",
    "\n",
    "* `gamma` refers to $\\gamma$, which you have seen before in the discounted sum of rewards.\n",
    "* `initial_entropy` refers to an exploration paramater, and more accurately, its value at the start of training.\n",
    "* `entropy_decay_rate` refers to the rate that the exploration parameter decays. This will be explained more concretely later.\n",
    "* `tensorboard_session_name` refers to the name that experiment logs will be saved with tensorboard. You will soon see how to use tensoboard.\n",
    "\n",
    "`returns` refers to the sum of rewards at each epoch. This is the value we intend to maximize, or in other words, this is a sample of $J(\\theta)$ when $\\gamma=1$. Why is $\\gamma$ set to one?\n",
    "Because our main goal has been optimizing the sum of rewards, without discounting. Discounting rewards is a trick we use to simplify training procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this code snippet (it takes 2 minutes on a MBP 2019), and observe the returns of an optimized and tuned pensieve training process. Notice how at first returns are small but after training they gradually increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = train_pensieve(gamma=0.9, ent_max=1, ent_decay=1/400, name='pensieve')\n",
    "plt.figure()\n",
    "plt.plot(ewma(ret, 0.97), color='C0')          # This is called an Exponentially Weighted Moving Average. It sounds complicated but it essentially smooths out noisy curves\n",
    "plt.plot(ret, alpha=0.2, color='C0')\n",
    "plt.ylim(bottom=-1500, top=1500)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('QoE Returns');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5\n",
    "Try decreasing and increasing gamma ($\\gamma$).\n",
    "You can try the extreme ends, namely $\\gamma=1, \\gamma=0$. \n",
    "\n",
    "Plot the returns for these schemes, and compare them to the original setting of $\\gamma=0.9$.\n",
    "\n",
    "Can you explain the difference based on what $\\gamma$ does?\n",
    "\n",
    "\n",
    "Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### START OF ASSIGNMENT 5 ###################################\n",
    "############# Your code below #################\n",
    "# ret_low_gamma = ...\n",
    "# ret_high_gamma = ...\n",
    "\n",
    "# plt.plot(...)\n",
    "########################### END OF ASSIGNMENT 5 #####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 6\n",
    "You observed an exploration scheme in Q-Learning, called epsilon-greedy. There is a similar thing in Actor Critic methods.\n",
    "\n",
    "In the Advantage Actor Critic setup that you saw before, we use entropy regularization to explore. The entropy of a policy, $H(\\pi)$ refers to how random it is. We want this entropy to be high at the start of training and gradually go down, so that by the end the policies are roughly deterministic.\n",
    "\n",
    "Towards this end, the entropy of the policy is added in the policy gradient loss with some coefficient $\\kappa$:\n",
    "$$loss = J(\\theta) + \\kappa \\cdot H(\\pi_\\theta)$$\n",
    "\n",
    "This $\\kappa$ determines how important increasing exploration is compared to optimizing returns. To motivate high exploration at the beginning and low exploration by the end, $\\kappa$ is annealed from some high value to zero in the training process.\n",
    "\n",
    "To see the effect of this entropy regularization scheme, try two variants:\n",
    "* No entropy regularization at all\n",
    "* No decaying the entropy\n",
    "\n",
    "Plot the returns for these schemes, and compare them to the original setting.\n",
    "\n",
    "Can you explain the difference based on what we described here?\n",
    "\n",
    "\n",
    "Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### START OF ASSIGNMENT 6 ###################################\n",
    "############# Your code below #################\n",
    "# ret_no_entropy = ...\n",
    "# ret_no_decay = ...\n",
    "\n",
    "# plt.plot(...)\n",
    "########################### END OF ASSIGNMENT 6 #####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "Tensorboard is an automatic and live visualization tool for ML and RL training. We log many values in the course of training, such as policy gradient loss, critic training loss, average action index, average rewards, average buffer in ABR sessions, etc. You can view all these logs, from environmental ones to RL training metrics and analyze/debug the training process.\n",
    "\n",
    "To use tensorboard, go to the `tensorboard` folder and in a terminal shell, run the following command:\n",
    "```\n",
    "tensorboard --logdir=. --port= 10234\n",
    "```\n",
    "Then in a web browser, go to this url and use tensorboard:\n",
    "[localhost:10234](http://localhost:10234/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyto] *",
   "language": "python",
   "name": "conda-env-pyto-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
